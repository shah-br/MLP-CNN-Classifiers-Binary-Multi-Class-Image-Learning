{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBAY5uDqn5U0",
        "outputId": "5ba54d87-49fa-4420-e573-e0af330830bb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from itertools import product\n",
        "\n",
        "# Check if a GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PK1Eb27dzOci",
        "outputId": "c6249246-4c82-4b52-89b0-7c1dc751e463"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and preprocess CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# CIFAR-10 class names\n",
        "class_names = [\n",
        "    \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\",\n",
        "    \"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\"\n",
        "]\n",
        "\n",
        "# Function to plot and save CIFAR-10 images\n",
        "def save_cifar10_visualization(images, labels, class_names, num_images=16, filename=\"cifar10_sample.png\"):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(class_names[labels[i][0]])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    print(f\"Visualization saved as {filename}\")\n",
        "\n",
        "# Save the visualization\n",
        "save_cifar10_visualization(x_train, y_train, class_names, num_images=16, filename=\"cifar10_sample.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNoRVo69x7uH"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess CIFAR-10 dataset\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2b7ujiQsyFL"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter options\n",
        "learning_rates = [0.05, 0.0001]\n",
        "kernel_sizes = [(3, 3), (7, 7)]\n",
        "batch_norm_options = [True, False]\n",
        "batch_sizes = [16, 256]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwLJGoaun7eM"
      },
      "outputs": [],
      "source": [
        "# Function to build and train the model\n",
        "def build_and_train_model(learning_rate, kernel_size, batch_norm, batch_size):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(layers.Conv2D(32, kernel_size, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    if batch_norm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "    if batch_norm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "\n",
        "    # First max pooling and dropout\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    # Third and fourth convolutional layers\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    if batch_norm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    if batch_norm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Second max pooling and dropout\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Fifth and sixth convolutional layers\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    if batch_norm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    if batch_norm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Third max pooling and dropout\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.4))\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train, y_train, epochs=15, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wH3jroToCN2",
        "outputId": "a0e48f4a-2dde-4ef2-cae6-e101c757e2f9"
      },
      "outputs": [],
      "source": [
        "# Iterate through all combinations of hyperparameters\n",
        "results = []\n",
        "for lr, ks, bn, bs in product(learning_rates, kernel_sizes, batch_norm_options, batch_sizes):\n",
        "    print(f\"Training with LR={lr}, Kernel={ks}, BatchNorm={bn}, BatchSize={bs}...\")\n",
        "    accuracy = build_and_train_model(lr, ks, bn, bs)\n",
        "    results.append((lr, ks, bn, bs, accuracy))\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Find and display the best accuracy\n",
        "best_result = max(results, key=lambda x: x[4])  # x[4] is the accuracy\n",
        "print(\"\\nBest Configuration:\")\n",
        "print(f\"Learning Rate: {best_result[0]}\")\n",
        "print(f\"Kernel Size: {best_result[1]}\")\n",
        "print(f\"Batch Normalization: {best_result[2]}\")\n",
        "print(f\"Batch Size: {best_result[3]}\")\n",
        "print(f\"Best Accuracy: {best_result[4]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
